{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import math\n",
    "import inspect\n",
    "import logging\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import imageio.v2 as imageio\n",
    "from enum import Enum\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from einops import rearrange\n",
    "from typing import Optional, Literal, Union\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from accelerate.logging import get_logger\n",
    "from deepspeed import DeepSpeedEngine\n",
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPModel, get_scheduler\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "\n",
    "from data.dataloader import CLIPDataset\n",
    "\n",
    "logger = get_logger(__name__, log_level=\"INFO\")\n",
    "\n"
   ],
   "id": "ee2a779774a78dcd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# tokenizer's parallelism issue\n",
    "# huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "# To disable this warning, you can either:\n",
    "# \t- Avoid using `tokenizers` before the fork if possible\n",
    "# \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)"
   ],
   "id": "34490f4f579153da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "os.environ['TOKENIZERS_PARALLELISM'] = 'false'",
   "id": "3564e4d77ee4352"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Copied from transformers.training_utils.SchedulerType\n",
    "# For explicitely enum scheduler types"
   ],
   "id": "6df5eb7f9754ca36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SchedulerType(Enum):\n",
    "    LINEAR = \"linear\"\n",
    "    COSINE = \"cosine\"\n",
    "    COSINE_WITH_RESTARTS = \"cosine_with_restarts\"\n",
    "    POLYNOMIAL = \"polynomial\"\n",
    "    CONSTANT = \"constant\"\n",
    "    CONSTANT_WITH_WARMUP = \"constant_with_warmup\"\n",
    "    INVERSE_SQRT = \"inverse_sqrt\"\n",
    "    REDUCE_ON_PLATEAU = \"reduce_lr_on_plateau\"\n"
   ],
   "id": "37b7b0d8d87039fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main(\n",
    "    *,\n",
    "    exp_name: Optional[str] = None,\n",
    "    data_root: str = './dataset',\n",
    "    pretrained_clip_path: str = 'openai/clip-vit-base-patch16',\n",
    "    use_lora: bool = False,\n",
    "    pretrained_lora_path: Optional[str] = None,\n",
    "    lora_config: Optional[dict] = None,\n",
    "    seed: Optional[int] = None,\n",
    "    train_split: Literal['train_raw', 'train_sub', 'train_all'] = 'train_raw',\n",
    "    learning_rate: float = 1e-5,\n",
    "    train_batch_size: int = 1,\n",
    "    val_batch_size: int = 1,\n",
    "    num_workers: int = 96,\n",
    "    max_train_steps: int = 1000,\n",
    "    adam_beta1: float = 0.9,\n",
    "    adam_beta2: float = 0.999,\n",
    "    adam_weight_decay: float = 1e-3,\n",
    "    adam_epsilon: float = 1e-08,\n",
    "    scheduler_type: Union[str, SchedulerType] = 'constant',\n",
    "    num_warmup_steps: Optional[int] = None,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    mixed_precision: Literal['no', 'fp16'] = 'fp16',\n",
    "    gradient_accumulation_steps: int = 1,\n",
    "    checkpointing_step_interv: int = 1000,\n",
    "    validation_step_interv: int = 500,\n",
    "    resume_from_checkpoint: Optional[str] = None,\n",
    "    output_dir: str = './output',\n",
    "):\n",
    "    # Get config\n",
    "    *_, config = inspect.getargvalues(inspect.currentframe())\n",
    "    exp_name = exp_name if exp_name is not None else datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "    output_dir = os.path.join(output_dir, exp_name)\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        mixed_precision=mixed_precision,\n",
    "        log_with='tensorboard',\n",
    "        project_dir=os.path.join(output_dir, 'logs')\n",
    "    )\n",
    "    # Make one log on every process with the configuration for debugging.\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(accelerator.state, main_process_only=False)\n",
    "    if accelerator.is_local_main_process:\n",
    "        transformers.utils.logging.set_verbosity_warning()\n",
    "    else:\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    # If passed along, set the training seed now.\n",
    "    if seed is not None:\n",
    "        set_seed(seed)\n",
    "\n",
    "    # Handle the output folder creation\n",
    "    if accelerator.is_main_process:\n",
    "        # now = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "        # output_dir = os.path.join(output_dir, now)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(f\"{output_dir}/validation\", exist_ok=True)\n",
    "        os.makedirs(f\"{output_dir}/pretrained\", exist_ok=True)\n",
    "        OmegaConf.save(config, os.path.join(output_dir, 'config.yaml'))\n"
   ],
   "id": "8361cdfc42c4b24c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "   # Load pretrained clip model\n",
    "    # model = CLIPModel.from_pretrained(pretrained_clip_path)\n",
    "    # processor = CLIPProcessor.from_pretrained(pretrained_clip_path)\n",
    "    # logger.info(f\"  Load pretrained CLIP model from {pretrained_clip_path}.\")\n"
   ],
   "id": "f12930ba2398b125"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    # mixed precision\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "\n",
    "    # Examine arguments\n",
    "    if pretrained_clip_path is None and pretrained_lora_path is None:\n",
    "        raise ValueError(\"No available pretrained CLIP path given!\")\n",
    "\n",
    "    if pretrained_clip_path is None:\n",
    "        assert use_lora, \"When fine-tuning full parameters, the pretrained CLIP path must be given.\"\n",
    "        peft_config = PeftConfig.from_pretrained(pretrained_lora_path)\n",
    "        pretrained_clip_path = peft_config.base_model_name_or_path\n",
    "\n",
    "    # Load CLIP model\n",
    "    model = CLIPModel.from_pretrained(pretrained_clip_path)\n",
    "    processor = CLIPProcessor.from_pretrained(pretrained_clip_path)\n",
    "    logger.info(f\"Load pretrained CLIP model from {pretrained_clip_path}.\")\n",
    "\n",
    "    # Maybe use lora\n",
    "    if use_lora:\n",
    "        model.to(dtype=weight_dtype)\n",
    "        logger.info(f\"  Use LoRA: {use_lora}, cast CLIP model weights to {weight_dtype}.\")\n",
    "        if pretrained_lora_path is not None:\n",
    "            model = PeftModel.from_pretrained(model, pretrained_lora_path, is_trainable=True)\n",
    "            logger.info(f\"  Use LoRA: {use_lora}, load pretrained LoRA model from {pretrained_lora_path}.\")\n",
    "        else:   # new lora layers\n",
    "            assert lora_config is not None, \"You are using LoRA fine-tuning and no `pretrained_lora_path` given, you should give a `lora_config` for initialization.\"\n",
    "            config = LoraConfig(\n",
    "                **lora_config\n",
    "            )\n",
    "            model = get_peft_model(model, config)\n",
    "            logger.info(f\"  Use LoRA: {use_lora}, no `pretrained_lora_path` provided, new LoRA layers are initialized.\")\n",
    "        if accelerator.is_main_process:\n",
    "            model.print_trainable_parameters()\n",
    "    # model.eval()    # frozen dropout and norm layers\n",
    "    model.train()\n"
   ],
   "id": "564cd359c1249e32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "    # Load dataset\n",
    "    train_dset = CLIPDataset(\n",
    "        root=data_root,\n",
    "        split=train_split,\n",
    "        processor=processor\n",
    "    )\n",
    "    train_dloader = DataLoader(\n",
    "        train_dset, batch_size=train_batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "    val_dset = CLIPDataset(root=data_root, split='val', processor=processor)\n",
    "    val_dloader = DataLoader(\n",
    "        val_dset, batch_size=val_batch_size\n",
    "    )\n",
    "\n",
    "    # Create optimizer\n",
    "    params = model.parameters()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params,\n",
    "        lr=learning_rate,\n",
    "        betas=(adam_beta1, adam_beta2),\n",
    "        weight_decay=adam_weight_decay,\n",
    "        eps=adam_epsilon\n",
    "    )\n",
    "    # lr scheduler\n",
    "    lr_scheduler = get_scheduler(\n",
    "        scheduler_type, optimizer,\n",
    "        num_warmup_steps * accelerator.num_processes,\n",
    "        max_train_steps * accelerator.num_processes\n",
    "    )\n",
    "\n",
    "    # Prepare everything with `accelerator`\n",
    "    model, optimizer, train_dloader, val_dloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dloader, val_dloader, lr_scheduler\n",
    "    )\n",
    "    model_dtype = model.dtype if hasattr(model, 'dtype') else model.module.dtype\n",
    "\n",
    "\n",
    "    # Move model to device and cast to weight_dtype\n",
    "    model.to(accelerator.device)\n",
    "\n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dloader) / gradient_accumulation_steps)\n",
    "    # Afterwards we recalculate our number of training epochs\n",
    "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    # We need to initialize the trackers we use, and also store our configuration.\n",
    "    # The trackers initializes automatically on the main process.\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(exp_name)\n",
    "\n",
    "    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dset)}\")\n",
    "    logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n",
    "    global_step = 0\n",
    "    first_epoch = 0\n",
    "\n",
    "    # Potentially load in the weights and states from a previous save\n",
    "    if resume_from_checkpoint:\n",
    "        if resume_from_checkpoint != \"latest\":\n",
    "            path = os.path.basename(resume_from_checkpoint)\n",
    "        else:\n",
    "            # Get the most recent checkpoint\n",
    "            dirs = os.listdir(output_dir)\n",
    "            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "            if len(dirs) > 0:\n",
    "                dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "                path = dirs[-1]\n",
    "            else:\n",
    "                path = None\n",
    "\n",
    "        if path is None:\n",
    "                accelerator.print(\n",
    "                    f\"Checkpoint '{resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "                )\n",
    "                resume_from_checkpoint = False\n",
    "        else:\n",
    "            accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "            accelerator.load_state(os.path.join(output_dir, path))\n",
    "            global_step = int(path.split(\"-\")[1])\n",
    "\n",
    "            first_epoch = global_step // num_update_steps_per_epoch\n",
    "            resume_step = global_step % num_update_steps_per_epoch\n",
    "            lr_scheduler.last_epoch = global_step\n",
    "\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(global_step, max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "\n",
    "    for epoch in range(first_epoch, num_train_epochs):\n",
    "        # model.train()\n",
    "        train_loss = 0.0\n",
    "        for step, batch in enumerate(train_dloader):\n",
    "            # Skip steps until we reach the resumed step\n",
    "            if resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n",
    "                if step % gradient_accumulation_steps == 0:\n",
    "                    progress_bar.update(1)\n",
    "                continue\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Get input\n",
    "                inputs = batch\n",
    "                # Forward\n",
    "                inputs = {k: v.to(accelerator.device) for k, v in inputs.items()}\n",
    "                inputs['pixel_values'] = inputs['pixel_values'].to(dtype=model_dtype)\n",
    "                inputs.update({'return_loss': True})\n",
    "                outputs = model(**inputs)\n",
    "                # Get loss\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # Gather the losses across all processes for logging (if we use distributed training).\n",
    "                avg_loss = accelerator.gather(loss.repeat(train_batch_size)).mean()\n",
    "                train_loss += avg_loss.item() / gradient_accumulation_steps\n",
    "\n",
    "                # Backpropagate\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(params, max_grad_norm)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "                train_loss = 0.0\n",
    "\n",
    "                logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "                progress_bar.set_postfix(**logs)\n",
    "\n",
    "                if global_step % checkpointing_step_interv == 0:\n",
    "                    if accelerator.is_main_process:\n",
    "                        save_path = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
    "                        accelerator.save_state(save_path)\n",
    "                        logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "                if global_step % validation_step_interv == 0:\n",
    "                    model.eval()\n",
    "                    # if accelerator.is_main_process:\n",
    "                    val_pbar = tqdm(val_dloader, leave=False, disable=not accelerator.is_local_main_process)\n",
    "                    val_pbar.set_description(\"Val\")\n",
    "                    val_losses = []\n",
    "                    for batch in val_dloader:    # only one batch\n",
    "                        # Get input\n",
    "                        inputs = batch\n",
    "\n",
    "                        # Forward\n",
    "                        inputs = {k: v.to(accelerator.device) for k, v in inputs.items()}\n",
    "                        inputs['pixel_values'] = inputs['pixel_values'].to(dtype=model_dtype)\n",
    "                        inputs.update({'return_loss': True})\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(**inputs)\n",
    "                        loss = outputs.loss\n",
    "\n",
    "                        # Gather the losses across all processes for logging (if we use distributed training).\n",
    "                        avg_loss = accelerator.gather(loss.repeat(val_batch_size)).mean()\n",
    "                        val_loss = avg_loss.item()\n",
    "                        val_losses.append(val_loss)\n",
    "\n",
    "                        # Logging\n",
    "                        val_pbar.update(1)\n",
    "\n",
    "                    val_mean_loss = np.mean(val_losses)\n",
    "                    accelerator.log({\"val_mean_loss\": val_mean_loss}, step=global_step)\n",
    "                    val_pbar.set_postfix({\"val_mean_loss\": val_mean_loss})\n",
    "                    model.train()\n",
    "\n",
    "\n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "    # Create the pipeline using the trained modules and save it.\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        model = accelerator.unwrap_model(model)\n",
    "        model.save_pretrained(f\"{output_dir}/pretrained\")\n",
    "        # Remember to save the processor\n",
    "        if not use_lora:\n",
    "            processor.save_pretrained(f\"{output_dir}/pretrained\")\n",
    "    accelerator.end_training()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config\", type=str, default=\"./configs/clip_ft_lora.yaml\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    config = OmegaConf.load(args.config)\n",
    "    config = OmegaConf.to_container(config)\n",
    "    main(**config)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "cbe5bc901c02b04"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
